RunPod VLM Dry Run - Quick Start
=================================

PURPOSE
-------
Test Linux-specific models (GPTQ/AWQ) and gated models (Llama/MiniCPM) on RunPod.

PREREQUISITES
-------------
1. RunPod API key in .env.local:
   RUNPOD_API_KEY=your_key_here

2. SSH key at ~/.ssh/id_rsa

3. Install dependencies:
   ./venv/Scripts/pip install runpod python-dotenv

USAGE
-----
./venv/Scripts/python experiments/runpod/run_dry_run.py

WHAT IT DOES
------------
1. Creates RTX 4090 pod (~$0.40/hr)
2. Installs PyTorch, Transformers, GPTQ/AWQ libraries
3. Uploads test script and test data
4. Tests 5 models:
   - hfl/Qwen2.5-VL-3B-Instruct-GPTQ-Int4
   - hfl/Qwen2.5-VL-7B-Instruct-GPTQ-Int4
   - Qwen/Qwen2.5-VL-32B-Instruct-AWQ
   - meta-llama/Llama-3.2-11B-Vision-Instruct
   - openbmb/MiniCPM-V-2_6
5. Downloads results to outputs/runpod_dry_run/results.json
6. Destroys pod

COST
----
~$0.40 (1 hour on RTX 4090)

FILES
-----
run_dry_run.py     - Main orchestration script (runs locally)
remote_test.py     - Test script uploaded to pod
runpod_instance.py - RunPod SDK wrapper
README.md          - Full documentation

TROUBLESHOOTING
---------------
See README.md for detailed troubleshooting guide.
