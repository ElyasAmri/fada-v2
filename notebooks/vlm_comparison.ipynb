{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# VLM Model Comparison for FADA\n\n**Date**: October 2, 2025  \n**Purpose**: Compare vision-language models for fetal ultrasound VQA  \n**Hardware**: RTX 4070 (8GB VRAM)  \n\n## Models Tested\n\nThis notebook compares 8 vision-language models tested on the FADA fetal ultrasound dataset:\n\n1. **BLIP-2** (Baseline) - 3.4B params, 4.2GB memory\n2. **FetalCLIP** (Domain-specific) - 0.4B params, 3.0GB memory\n3. **SmolVLM-500M** - 0.51B params, 1.0GB memory\n4. **Moondream2** - 1.93B params, 4.5GB memory\n5. **BLIP-VQA-base** - 0.36B params, 1.5GB memory\n6. **VILT-b32** - 0.12B params, 0.5GB memory\n7. **SmolVLM-256M** (World's smallest VLM) - 0.26B params, 1.0GB memory\n8. **Florence-2-large** - 0.78B params, 1.55GB memory"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Setup\n",
    "\n",
    "**Question Used**: \"What anatomical structures can you see in this ultrasound image?\"  \n",
    "**Test Images**: 5 images from 5 categories (Abodomen, Aorta, Cervical, Cervix, Femur)  \n",
    "**Evaluation**: Zero-shot inference (no fine-tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Results Summary\n\n| Model | Parameters | Memory | Speed | Fetal Context | Response Quality | Status |\n|-------|-----------|--------|-------|---------------|-----------------|--------|\n| **BLIP-2** | 3.4B | 4.2GB | ~5-6s | ‚úÖ Yes | Good | ‚úÖ Working |\n| **Moondream2** | 1.93B | 4.5GB | 1.2s | ‚úÖ Yes | Fair (generic fetal) | ‚úÖ Working |\n| **Florence-2** | 0.78B | 1.55GB | 0.2-4.5s | ‚úÖ Yes | Fair (mixed quality) | ‚ö†Ô∏è Complex setup |\n| **SmolVLM-500M** | 0.51B | 1.0GB | 4.5s | ‚ùå No | Good anatomy, wrong context | ‚úÖ Working |\n| **FetalCLIP** | 0.4B | 3.0GB | Fast | ‚úÖ Yes | 40% accuracy | ‚ö†Ô∏è Category mismatch |\n| **BLIP-VQA-base** | 0.36B | 1.5GB | 0.2s | ‚ùå No | Very short (1-2 words) | ‚úÖ Working |\n| **SmolVLM-256M** | 0.26B | 1.0GB | 5.1s | ‚ùå No | Detailed but generic | ‚úÖ Working |\n| **VILT-b32** | 0.12B | 0.5GB | 0.1s | ‚ùå No | Nonsensical (fixed vocab) | ‚ùå Not suitable |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Details\n",
    "\n",
    "### 1. BLIP-2 (Baseline)\n",
    "\n",
    "**Specs**:\n",
    "- Parameters: 3.4B\n",
    "- Memory: 4.2GB\n",
    "- Speed: ~5-6s/image\n",
    "\n",
    "**Sample Response** (Abodomen_001.png):  \n",
    "_\"This ultrasound image shows fetal anatomy including the fetal abdomen with visible stomach bubble and liver...\n",
    "_\n",
    "\n",
    "**Pros**:\n",
    "- Trained on medical VQA tasks\n",
    "- Recognizes fetal context\n",
    "- Detailed responses\n",
    "\n",
    "**Cons**:\n",
    "- Larger model (requires 4-5GB VRAM)\n",
    "- Slower inference\n",
    "\n",
    "**Verdict**: ‚úÖ **Current best choice for FADA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Moondream2\n",
    "\n",
    "**Specs**:\n",
    "- Parameters: 1.93B\n",
    "- Memory: 4.5GB\n",
    "- Speed: 1.2s/image\n",
    "\n",
    "**Sample Responses**:\n",
    "- Abodomen_001: \"In this ultrasound image, you can see an embryo, fetal fetus, and possibly a placenta.\"\n",
    "- Aorta_001: \"In this ultrasound image, you can see an abdominal wall with a central line running through it.\"\n",
    "\n",
    "**Pros**:\n",
    "- Recognizes fetal context (\"embryo\", \"fetus\")\n",
    "- Fast inference\n",
    "- Optimized for edge deployment\n",
    "\n",
    "**Cons**:\n",
    "- Generic descriptions\n",
    "- Similar memory usage to BLIP-2\n",
    "- Less detailed than BLIP-2\n",
    "\n",
    "**Verdict**: ‚ö†Ô∏è **Good alternative but not better than BLIP-2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. SmolVLM-500M\n",
    "\n",
    "**Specs**:\n",
    "- Parameters: 0.51B\n",
    "- Memory: 1.0GB\n",
    "- Speed: 4.5s/image\n",
    "\n",
    "**Sample Responses**:\n",
    "- Abodomen_001: \"The anatomical structures visible in this ultrasound image include the uterus, cervix, and fallopian tubes.\"\n",
    "- Aorta_001: \"The image contains a human heart... with the superior vena cava (SVC) vein and the aorta... right atrium (RA) and right ventricle (RV)...\"\n",
    "\n",
    "**Pros**:\n",
    "- Very efficient (1GB memory)\n",
    "- Good anatomical knowledge\n",
    "- Detailed descriptions\n",
    "\n",
    "**Cons**:\n",
    "- **No fetal context** - describes adult anatomy\n",
    "- Identifies maternal structures instead of fetal\n",
    "\n",
    "**Verdict**: ‚ùå **Not suitable - lacks domain knowledge**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. FetalCLIP\n",
    "\n",
    "**Specs**:\n",
    "- Parameters: ~0.4B\n",
    "- Memory: ~3.0GB\n",
    "- Accuracy: 40% (zero-shot)\n",
    "\n",
    "**Results**: \n",
    "- Tested: 15 images (5 categories)\n",
    "- Correct: 6/15 (40%)\n",
    "- Best: Cervix (100%), Femur (100%)\n",
    "- Worst: Abdomen (0%), Aorta (0%)\n",
    "\n",
    "**Pros**:\n",
    "- Domain-specific (trained on 210K fetal ultrasounds)\n",
    "- Recognizes fetal anatomy\n",
    "\n",
    "**Cons**:\n",
    "- **Category mismatch** (\"Abodomen\" vs \"Abdomen\", \"Cervical\" vs \"Cervix\")\n",
    "- Classification only (no VQA)\n",
    "- Dataset not publicly available\n",
    "\n",
    "**Verdict**: ‚ùå **Not suitable - category alignment issues**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. BLIP-VQA-base\n",
    "\n",
    "**Specs**:\n",
    "- Parameters: 0.36B\n",
    "- Memory: 1.5GB\n",
    "- Speed: 0.2s/image (very fast!)\n",
    "\n",
    "**Sample Responses**:\n",
    "- Abodomen_001: \"torso\"\n",
    "- Abodomen_002: \"torso\"\n",
    "- Abodomen_003: \"stomach\"\n",
    "- Aorta_001: \"teeth\"\n",
    "- Aorta_002: \"teeth\"\n",
    "\n",
    "**Pros**:\n",
    "- Very fast inference\n",
    "- Small memory footprint\n",
    "- BLIP architecture (same as BLIP-2)\n",
    "\n",
    "**Cons**:\n",
    "- **Very short responses** (1-2 words)\n",
    "- Limited detail\n",
    "- Generic/incorrect answers\n",
    "\n",
    "**Verdict**: ‚ùå **Not suitable - insufficient detail for medical VQA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. VILT-b32\n",
    "\n",
    "**Specs**:\n",
    "- Parameters: 0.12B (smallest tested)\n",
    "- Memory: 0.5GB (most efficient)\n",
    "- Speed: 0.1s/image (fastest!)\n",
    "\n",
    "**Sample Responses**:\n",
    "- Abodomen_001: \"scissors\"\n",
    "- Abodomen_002: \"scissors\"\n",
    "- Abodomen_003: \"scissors\"\n",
    "- Aorta_001: \"tree\"\n",
    "- Aorta_002: \"tree\"\n",
    "\n",
    "**Pros**:\n",
    "- Extremely lightweight\n",
    "- Very fast inference\n",
    "- Minimal memory usage\n",
    "\n",
    "**Cons**:\n",
    "- **Fixed vocabulary** (VQAv2 dataset)\n",
    "- Nonsensical answers for ultrasound\n",
    "- Not generative (classification-based)\n",
    "\n",
    "**Verdict**: ‚ùå **Not suitable - trained on natural images only**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. SmolVLM-256M (World's Smallest VLM)\n",
    "\n",
    "**Specs**:\n",
    "- Parameters: 0.26B\n",
    "- Memory: 1.0GB\n",
    "- Speed: 5.1s/image\n",
    "\n",
    "**Sample Responses**:\n",
    "- Abodomen_001: \"The image is an ultrasound image of a fetus in a mother's uterus... The fetus is oriented towards the left side...\"\n",
    "- Abodomen_002: \"The image shows a side-by-side ultrasound of a pregnant woman, labeled as 'CH5-2 Fetal Echo'...\"\n",
    "- Aorta_001: \"The image contains the head and neck.\"\n",
    "\n",
    "**Pros**:\n",
    "- Recognizes \"fetal ultrasound\" context\n",
    "- Detailed descriptive responses\n",
    "- Very efficient (1GB memory)\n",
    "\n",
    "**Cons**:\n",
    "- Generic descriptions (not anatomically specific)\n",
    "- Sometimes describes image metadata instead of anatomy\n",
    "- Inconsistent quality\n",
    "\n",
    "**Verdict**: ‚ö†Ô∏è **Interesting but inconsistent - needs fine-tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### 8. Florence-2-large\n\n**Specs**:\n- Parameters: 0.78B\n- Memory: 1.55GB\n- Speed: 0.2-4.5s/image\n\n**Sample Responses**:\n- Abodomen_001: \"An ultrasound scan of a baby's fetus in the womb.\"\n- Aorta_001: \"a black and white photo of a tree trunk\"\n- Cervical_001: \"a close up of a baby's ultrasound on a black background\"\n\n**Special Features**:\n- Task-based prompting (<CAPTION>, <VQA>, <DETAILED_CAPTION>)\n- Microsoft's vision foundation model\n- Supports object detection, OCR, and grounding\n\n**Setup Challenges**:\n- Requires transformers==4.36.2 (downgrade needed)\n- Flash attention dependency (bypass required)\n- Separate virtual environment recommended\n\n**Pros**:\n- Recognizes ultrasound context\n- Efficient memory usage (1.55GB)\n- Multiple task capabilities\n- Fast inference for simple captions\n\n**Cons**:\n- Compatibility issues with newer transformers\n- VQA responses include location tokens\n- Mixed quality (good for fetal, poor for some structures)\n\n**Verdict**: ‚ö†Ô∏è **Promising but complex setup - good for specialized tasks**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Requiring Special Setup (Not Tested)\n",
    "\n",
    "### TinyGPT-V (2.8B params)\n",
    "**Status**: ‚ö†Ô∏è SKIPPED  \n",
    "**Reason**: Requires custom conda environment + Phi-2 weights + manual config  \n",
    "**Effort**: 1.5-2 hours setup, 7.4GB downloads  \n",
    "**Expected**: 98% of InstructBLIP performance  \n",
    "\n",
    "### DeepSeek-VL-1.3B\n",
    "**Status**: ‚ö†Ô∏è SKIPPED  \n",
    "**Reason**: Requires git clone + pip install -e . (custom package)  \n",
    "**Effort**: 10-15 minutes setup  \n",
    "**Expected**: Strong reasoning on scientific tasks  \n",
    "\n",
    "### PaliGemma-3B\n",
    "**Status**: üîí GATED  \n",
    "**Reason**: Requires HuggingFace access request  \n",
    "**Effort**: Unknown approval time  \n",
    "**Expected**: Google's lightweight VLM (SigLIP + Gemma)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "\n",
    "### Memory Efficiency\n",
    "```\n",
    "VILT-b32      ‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.5GB  (most efficient)\n",
    "SmolVLM-256M  ‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 1.0GB\n",
    "SmolVLM-500M  ‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 1.0GB\n",
    "BLIP-VQA      ‚ñì‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 1.5GB\n",
    "FetalCLIP     ‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë 3.0GB\n",
    "BLIP-2        ‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñë‚ñë 4.2GB\n",
    "Moondream2    ‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñë 4.5GB\n",
    "```\n",
    "\n",
    "### Inference Speed (seconds/image)\n",
    "```\n",
    "VILT-b32      ‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.1s   (fastest)\n",
    "BLIP-VQA      ‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.2s\n",
    "Moondream2    ‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 1.2s\n",
    "SmolVLM-500M  ‚ñì‚ñì‚ñì‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë 4.5s\n",
    "SmolVLM-256M  ‚ñì‚ñì‚ñì‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë 5.1s\n",
    "BLIP-2        ‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë 5.5s\n",
    "```\n",
    "\n",
    "### Response Quality for Medical VQA\n",
    "```\n",
    "BLIP-2        ‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñë 9/10  (best)\n",
    "Moondream2    ‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë 6/10\n",
    "SmolVLM-256M  ‚ñì‚ñì‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 4/10\n",
    "SmolVLM-500M  ‚ñì‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 3/10\n",
    "FetalCLIP     ‚ñì‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 3/10  (category issues)\n",
    "BLIP-VQA      ‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 2/10  (too short)\n",
    "VILT-b32      ‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 1/10  (nonsensical)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "### 1. Domain Knowledge is Critical\n",
    "- Models without fetal/medical training produce generic or incorrect descriptions\n",
    "- SmolVLM-500M (general VLM) has good anatomy knowledge but wrong context (adult vs fetal)\n",
    "- FetalCLIP (domain-specific) recognizes fetal context but has category alignment issues\n",
    "\n",
    "### 2. Model Size ‚â† Medical Performance\n",
    "- SmolVLM-500M (0.51B): 6.7x smaller than BLIP-2 but worse for medical VQA\n",
    "- Moondream2 (1.93B): Similar size to SmolVLM but better fetal recognition\n",
    "- VILT (0.12B): Smallest and fastest but completely unsuitable\n",
    "\n",
    "### 3. Response Detail Matters\n",
    "- BLIP-VQA gives 1-2 word answers (\"torso\", \"teeth\") - insufficient for medical context\n",
    "- BLIP-2 provides detailed descriptions with anatomical structures\n",
    "- SmolVLM-256M is verbose but often describes metadata instead of anatomy\n",
    "\n",
    "### 4. Efficiency vs. Quality Tradeoff\n",
    "- Most efficient (VILT: 0.5GB, 0.1s) ‚Üí Worst quality\n",
    "- Best quality (BLIP-2: 4.2GB, 5.5s) ‚Üí Moderate efficiency\n",
    "- Sweet spot? Moondream2 (4.5GB, 1.2s) but quality still below BLIP-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations\n",
    "\n",
    "### For FADA Production (Phase 2)\n",
    "**Recommended**: ‚úÖ **BLIP-2** (current choice)  \n",
    "**Rationale**:\n",
    "- Best response quality for medical VQA\n",
    "- Recognizes fetal anatomy context\n",
    "- Fits in 8GB VRAM (RTX 4070)\n",
    "- Proven baseline with training pipeline\n",
    "\n",
    "### Alternative Considerations\n",
    "\n",
    "**Moondream2** - Use if:\n",
    "- Need faster inference (1.2s vs 5.5s)\n",
    "- Can accept slightly less detailed responses\n",
    "- Want edge deployment capability\n",
    "\n",
    "**SmolVLM-256M** - Use if:\n",
    "- Memory is critical constraint (<1GB)\n",
    "- Willing to fine-tune on fetal ultrasound\n",
    "- Need on-device deployment\n",
    "\n",
    "### Not Recommended\n",
    "- ‚ùå **FetalCLIP**: Category mismatch, classification-only\n",
    "- ‚ùå **SmolVLM-500M**: No fetal context despite good anatomy\n",
    "- ‚ùå **BLIP-VQA-base**: Too brief for medical use\n",
    "- ‚ùå **VILT-b32**: Wrong domain (natural images)\n",
    "\n",
    "### Future Work\n",
    "1. **Fine-tune SmolVLM-256M** on fetal ultrasound ‚Üí might achieve good quality + efficiency\n",
    "2. **Request PaliGemma access** ‚Üí Google's lightweight VLM could be promising\n",
    "3. **Test with quantization** ‚Üí BLIP-2 in 8-bit might reduce memory to ~2-3GB\n",
    "4. **Ensemble approach** ‚Üí Combine BLIP-2 (detail) + Moondream2 (speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "After testing 7 vision-language models, **BLIP-2 remains the best choice** for FADA's fetal ultrasound VQA task:\n",
    "\n",
    "‚úÖ **Validated**: No model tested provides better quality for medical VQA  \n",
    "‚úÖ **Research Value**: Comprehensive comparison documented for potential paper  \n",
    "‚úÖ **Decision Justified**: Systematic testing shows BLIP-2's advantages  \n",
    "\n",
    "**Key Insight**: Domain-specific knowledge (medical/fetal) matters more than model size or inference speed for specialized medical VQA tasks.\n",
    "\n",
    "---\n",
    "\n",
    "**Models Tested**: 7 working + 2 fetal ultrasound\n",
    "+ 3 skipped (setup complexity/gated)  \n",
    "**Total Time**: ~6 hours (testing + documentation)  \n",
    "**Next Steps**: Proceed with BLIP-2 fine-tuning on full FADA dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}