{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12-Class Fetal Ultrasound Classification\n",
    "\n",
    "This notebook trains a 12-class classifier for fetal ultrasound images with:\n",
    "- Stratified splits to ensure all classes in train/val/test\n",
    "- Focal loss to handle 5:1 class imbalance\n",
    "- Comprehensive metrics (balanced accuracy, macro F1, Cohen's Kappa)\n",
    "- MLflow experiment tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nsys.path.append('..')\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.cuda.amp import GradScaler, autocast\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nimport mlflow\nimport mlflow.pytorch\nfrom sklearn.metrics import (\n    accuracy_score, balanced_accuracy_score, f1_score,\n    cohen_kappa_score, classification_report, confusion_matrix\n)\n\nfrom src.data.dataset import FetalDataModule12Class\nfrom src.data.augmentation import get_training_augmentation, get_validation_augmentation\nfrom src.models.classifier import create_model\nfrom src.utils.training import EarlyStopping, ModelCheckpoint\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set style\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 8)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Data\n",
    "    data_root = '../data/Fetal Ultrasound'\n",
    "    image_size = 224\n",
    "    num_classes = 12\n",
    "    \n",
    "    # Model\n",
    "    backbone = 'efficientnet_b0'  # Options: efficientnet_b0, resnet18, densenet121, mobilenet_v2\n",
    "    pretrained = True\n",
    "    dropout = 0.2\n",
    "    \n",
    "    # Training\n",
    "    batch_size = 32\n",
    "    epochs = 30  # Reduced for initial testing\n",
    "    learning_rate = 1e-3\n",
    "    weight_decay = 1e-4\n",
    "    \n",
    "    # Loss\n",
    "    use_focal_loss = True\n",
    "    focal_gamma = 2.0\n",
    "    \n",
    "    # Early stopping\n",
    "    patience = 7\n",
    "    \n",
    "    # System\n",
    "    num_workers = 0  # Set to 0 for Windows\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    seed = 42\n",
    "    \n",
    "    # MLflow\n",
    "    use_mlflow = True\n",
    "    experiment_name = '12-class-ultrasound'\n",
    "\n",
    "config = Config()\n",
    "print(f\"Using device: {config.device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set Random Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(config.seed)\n",
    "print(\"Random seeds set for reproducibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data module\n",
    "print(\"Initializing data module...\")\n",
    "data_module = FetalDataModule12Class(\n",
    "    data_root=config.data_root,\n",
    "    batch_size=config.batch_size,\n",
    "    num_workers=config.num_workers,\n",
    "    random_state=config.seed\n",
    ")\n",
    "\n",
    "# Setup with augmentations\n",
    "data_module.setup(\n",
    "    train_transform=get_training_augmentation(config.image_size),\n",
    "    val_transform=get_validation_augmentation(config.image_size)\n",
    ")\n",
    "\n",
    "# Get dataloaders\n",
    "train_loader = data_module.train_dataloader()\n",
    "val_loader = data_module.val_dataloader()\n",
    "test_loader = data_module.test_dataloader()\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Train: {len(data_module.train_dataset)} samples\")\n",
    "print(f\"  Val: {len(data_module.val_dataset)} samples\")\n",
    "print(f\"  Test: {len(data_module.test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class distribution\n",
    "train_dist = data_module.train_dataset.get_class_distribution()\n",
    "class_names = data_module.train_dataset.CLASSES\n",
    "\n",
    "# Plot distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar plot\n",
    "classes = list(train_dist.keys())\n",
    "counts = list(train_dist.values())\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(classes)))\n",
    "\n",
    "bars = ax1.bar(range(len(classes)), counts, color=colors)\n",
    "ax1.set_xticks(range(len(classes)))\n",
    "ax1.set_xticklabels(classes, rotation=45, ha='right')\n",
    "ax1.set_ylabel('Number of Images')\n",
    "ax1.set_title('Training Set Class Distribution')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add count labels on bars\n",
    "for bar, count in zip(bars, counts):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{count}', ha='center', va='bottom')\n",
    "\n",
    "# Pie chart\n",
    "ax2.pie(counts, labels=classes, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "ax2.set_title('Training Set Class Proportions')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print class weights\n",
    "print(\"\\nClass Weights (for balancing):\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    weight = data_module.class_weights[i].item()\n",
    "    print(f\"  {class_name:30s}: {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Model and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model with focal loss and class weights\n",
    "print(f\"Creating model: {config.backbone}\")\n",
    "model, criterion = create_model(\n",
    "    num_classes=config.num_classes,\n",
    "    backbone=config.backbone,\n",
    "    pretrained=config.pretrained,\n",
    "    dropout_rate=config.dropout,\n",
    "    class_weights=data_module.class_weights.to(config.device),\n",
    "    use_focal_loss=config.use_focal_loss,\n",
    "    focal_gamma=config.focal_gamma\n",
    ")\n",
    "\n",
    "model = model.to(config.device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nModel Parameters:\")\n",
    "print(f\"  Total: {total_params:,}\")\n",
    "print(f\"  Trainable: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Setup Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=config.epochs,\n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "# Mixed precision scaler for faster training\n",
    "scaler = GradScaler()\n",
    "\n",
    "print(f\"Optimizer: AdamW (lr={config.learning_rate}, wd={config.weight_decay})\")\n",
    "print(f\"Scheduler: CosineAnnealingLR\")\n",
    "print(f\"Mixed Precision: Enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Initialize Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=config.patience,\n",
    "    mode='max',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Model checkpoint\n",
    "Path('../models').mkdir(exist_ok=True)\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=f'../models/best_model_{config.backbone}_12class.pth',\n",
    "    monitor='balanced_accuracy',\n",
    "    mode='max',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# MLflow\n",
    "if config.use_mlflow:\n",
    "    mlflow.set_experiment(config.experiment_name)\n",
    "    mlflow.start_run()\n",
    "    \n",
    "    # Log configuration\n",
    "    mlflow.log_params({\n",
    "        'backbone': config.backbone,\n",
    "        'pretrained': config.pretrained,\n",
    "        'batch_size': config.batch_size,\n",
    "        'learning_rate': config.learning_rate,\n",
    "        'epochs': config.epochs,\n",
    "        'image_size': config.image_size,\n",
    "        'use_focal_loss': config.use_focal_loss,\n",
    "        'focal_gamma': config.focal_gamma,\n",
    "        'dropout': config.dropout,\n",
    "        'weight_decay': config.weight_decay,\n",
    "        'num_classes': config.num_classes\n",
    "    })\n",
    "    print(\"MLflow tracking started\")\n",
    "\n",
    "print(\"Tracking initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, scaler, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc='Training', leave=False)\n",
    "    for batch_idx, (images, labels) in enumerate(progress_bar):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Mixed precision training\n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "    epoch_balanced_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return epoch_loss, epoch_acc, epoch_balanced_acc\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc='Validation', leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    \n",
    "    # Calculate comprehensive metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(all_labels, all_preds),\n",
    "        'balanced_accuracy': balanced_accuracy_score(all_labels, all_preds),\n",
    "        'macro_f1': f1_score(all_labels, all_preds, average='macro'),\n",
    "        'weighted_f1': f1_score(all_labels, all_preds, average='weighted'),\n",
    "        'cohen_kappa': cohen_kappa_score(all_labels, all_preds)\n",
    "    }\n",
    "    \n",
    "    return epoch_loss, metrics, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [], 'train_balanced_acc': [],\n",
    "    'val_loss': [], 'val_acc': [], 'val_balanced_acc': [],\n",
    "    'val_macro_f1': [], 'val_cohen_kappa': []\n",
    "}\n",
    "\n",
    "best_balanced_acc = 0.0\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{config.epochs}\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc, train_balanced_acc = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, scaler, config.device\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_metrics, val_preds, val_labels = validate(\n",
    "        model, val_loader, criterion, config.device\n",
    "    )\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_balanced_acc'].append(train_balanced_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_metrics['accuracy'])\n",
    "    history['val_balanced_acc'].append(val_metrics['balanced_accuracy'])\n",
    "    history['val_macro_f1'].append(val_metrics['macro_f1'])\n",
    "    history['val_cohen_kappa'].append(val_metrics['cohen_kappa'])\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, Balanced Acc: {train_balanced_acc:.4f}\")\n",
    "    print(f\"Val   - Loss: {val_loss:.4f}, Acc: {val_metrics['accuracy']:.4f}, Balanced Acc: {val_metrics['balanced_accuracy']:.4f}\")\n",
    "    print(f\"        Macro F1: {val_metrics['macro_f1']:.4f}, Cohen's Kappa: {val_metrics['cohen_kappa']:.4f}\")\n",
    "    \n",
    "    # MLflow logging\n",
    "    if config.use_mlflow:\n",
    "        mlflow.log_metrics({\n",
    "            'train_loss': train_loss,\n",
    "            'train_accuracy': train_acc,\n",
    "            'train_balanced_accuracy': train_balanced_acc,\n",
    "            'val_loss': val_loss,\n",
    "            **{f'val_{k}': v for k, v in val_metrics.items()},\n",
    "            'learning_rate': optimizer.param_groups[0]['lr']\n",
    "        }, step=epoch)\n",
    "    \n",
    "    # Checkpoint and early stopping\n",
    "    checkpoint(val_metrics['balanced_accuracy'], model, optimizer, epoch)\n",
    "    \n",
    "    if early_stopping(val_metrics['balanced_accuracy']):\n",
    "        print(\"\\nEarly stopping triggered!\")\n",
    "        break\n",
    "    \n",
    "    # Update best score\n",
    "    if val_metrics['balanced_accuracy'] > best_balanced_acc:\n",
    "        best_balanced_acc = val_metrics['balanced_accuracy']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Training complete! Best validation balanced accuracy: {best_balanced_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history['train_loss'], label='Train', linewidth=2)\n",
    "axes[0, 0].plot(history['val_loss'], label='Validation', linewidth=2)\n",
    "axes[0, 0].set_title('Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 1].plot(history['train_acc'], label='Train', linewidth=2)\n",
    "axes[0, 1].plot(history['val_acc'], label='Validation', linewidth=2)\n",
    "axes[0, 1].set_title('Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Balanced Accuracy\n",
    "axes[0, 2].plot(history['train_balanced_acc'], label='Train', linewidth=2)\n",
    "axes[0, 2].plot(history['val_balanced_acc'], label='Validation', linewidth=2)\n",
    "axes[0, 2].set_title('Balanced Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 2].set_xlabel('Epoch')\n",
    "axes[0, 2].set_ylabel('Balanced Accuracy')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Macro F1\n",
    "axes[1, 0].plot(history['val_macro_f1'], label='Validation Macro F1', linewidth=2, color='green')\n",
    "axes[1, 0].set_title('Macro F1 Score', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Macro F1')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cohen's Kappa\n",
    "axes[1, 1].plot(history['val_cohen_kappa'], label=\"Cohen's Kappa\", linewidth=2, color='purple')\n",
    "axes[1, 1].set_title(\"Cohen's Kappa\", fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Kappa')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning Rate\n",
    "lrs = [optimizer.param_groups[0]['lr']] * len(history['train_loss'])\n",
    "axes[1, 2].plot(lrs, linewidth=2, color='orange')\n",
    "axes[1, 2].set_title('Learning Rate', fontsize=14, fontweight='bold')\n",
    "axes[1, 2].set_xlabel('Epoch')\n",
    "axes[1, 2].set_ylabel('LR')\n",
    "axes[1, 2].set_yscale('log')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Load Best Model and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint_path = f'../models/best_model_{config.backbone}_12class.pth'\n",
    "if Path(checkpoint_path).exists():\n",
    "    checkpoint_dict = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint_dict['model_state_dict'])\n",
    "    print(f\"Loaded best model from epoch {checkpoint_dict['epoch']+1}\")\n",
    "else:\n",
    "    print(\"No checkpoint found, using current model\")\n",
    "\n",
    "# Test evaluation\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_loss, test_metrics, test_preds, test_labels = validate(\n",
    "    model, test_loader, criterion, config.device\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"\\nTest Metrics:\")\n",
    "for metric_name, value in test_metrics.items():\n",
    "    print(f\"  {metric_name:20s}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class_names = data_module.train_dataset.CLASSES\n",
    "report = classification_report(test_labels, test_preds, target_names=class_names, digits=3)\n",
    "print(report)\n",
    "\n",
    "# Save report\n",
    "report_dict = classification_report(\n",
    "    test_labels, test_preds,\n",
    "    target_names=class_names,\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "report_df = pd.DataFrame(report_dict).transpose()\n",
    "report_df = report_df.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Test Set', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Predicted Class', fontsize=12)\n",
    "plt.ylabel('True Class', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Normalized confusion matrix\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='YlOrRd',\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Percentage'})\n",
    "plt.title('Normalized Confusion Matrix - Test Set', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Predicted Class', fontsize=12)\n",
    "plt.ylabel('True Class', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Per-Class Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract per-class metrics\n",
    "per_class_metrics = []\n",
    "for i, class_name in enumerate(class_names):\n",
    "    if class_name in report_dict:\n",
    "        metrics = report_dict[class_name]\n",
    "        per_class_metrics.append({\n",
    "            'Class': class_name,\n",
    "            'Precision': metrics['precision'],\n",
    "            'Recall': metrics['recall'],\n",
    "            'F1-Score': metrics['f1-score'],\n",
    "            'Support': int(metrics['support'])\n",
    "        })\n",
    "\n",
    "metrics_df = pd.DataFrame(per_class_metrics)\n",
    "\n",
    "# Plot per-class performance\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Precision\n",
    "axes[0].barh(metrics_df['Class'], metrics_df['Precision'], color='skyblue')\n",
    "axes[0].set_xlabel('Precision')\n",
    "axes[0].set_title('Per-Class Precision', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlim([0, 1])\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Recall\n",
    "axes[1].barh(metrics_df['Class'], metrics_df['Recall'], color='lightgreen')\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_title('Per-Class Recall', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlim([0, 1])\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# F1-Score\n",
    "axes[2].barh(metrics_df['Class'], metrics_df['F1-Score'], color='salmon')\n",
    "axes[2].set_xlabel('F1-Score')\n",
    "axes[2].set_title('Per-Class F1-Score', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlim([0, 1])\n",
    "axes[2].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display metrics table\n",
    "print(\"\\nPer-Class Performance:\")\n",
    "print(metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most confused class pairs\n",
    "confusion_pairs = []\n",
    "for i in range(len(class_names)):\n",
    "    for j in range(len(class_names)):\n",
    "        if i != j and cm[i, j] > 0:\n",
    "            confusion_pairs.append({\n",
    "                'True': class_names[i],\n",
    "                'Predicted': class_names[j],\n",
    "                'Count': cm[i, j],\n",
    "                'Percentage': cm_normalized[i, j] * 100\n",
    "            })\n",
    "\n",
    "confusion_df = pd.DataFrame(confusion_pairs)\n",
    "confusion_df = confusion_df.sort_values('Count', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP CONFUSION PAIRS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nMost confused class pairs (top 10):\")\n",
    "print(confusion_df.head(10).to_string(index=False))\n",
    "\n",
    "# Classes with lowest performance\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLASSES NEEDING IMPROVEMENT\")\n",
    "print(\"=\"*80)\n",
    "worst_classes = metrics_df.nsmallest(5, 'F1-Score')\n",
    "print(\"\\nClasses with lowest F1-scores:\")\n",
    "print(worst_classes.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Save Results and Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results = {\n",
    "    'config': vars(config),\n",
    "    'test_metrics': test_metrics,\n",
    "    'best_val_balanced_acc': best_balanced_acc,\n",
    "    'training_history': history,\n",
    "    'classification_report': report_dict\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../outputs/12class_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Results saved to outputs/12class_results.json\")\n",
    "\n",
    "# MLflow cleanup\n",
    "if config.use_mlflow:\n",
    "    # Log final test metrics\n",
    "    for metric_name, value in test_metrics.items():\n",
    "        mlflow.log_metric(f'test_{metric_name}', value)\n",
    "    \n",
    "    # Save artifacts\n",
    "    mlflow.log_artifact('../outputs/12class_results.json')\n",
    "    \n",
    "    mlflow.end_run()\n",
    "    print(\"MLflow run completed\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Final Test Balanced Accuracy: {test_metrics['balanced_accuracy']:.4f}\")\n",
    "print(f\"Final Test Macro F1: {test_metrics['macro_f1']:.4f}\")\n",
    "print(f\"Final Test Cohen's Kappa: {test_metrics['cohen_kappa']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}