{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Florence-2 VQA Pipeline Test\n",
    "\n",
    "**Model**: `microsoft/Florence-2-base`  \n",
    "**Task**: Visual Question Answering (VQA) on fetal ultrasound images  \n",
    "**Status**: ❌ FAILED - Compatibility issues with SDPA and dtype\n",
    "\n",
    "This notebook documents an attempt to use Microsoft's Florence-2 base model for VQA on fetal ultrasound images from the FADA dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Excel annotation file\n",
    "DATA_DIR = Path(r\"C:\\Users\\elyas\\Workspace\\PyCharm\\fada-v3\\data\\Fetal Ultrasound Labeled\")\n",
    "IMAGE_DIR = Path(r\"C:\\Users\\elyas\\Workspace\\PyCharm\\fada-v3\\data\\Fetal Ultrasound\\Non_standard_NT\")\n",
    "\n",
    "excel_path = DATA_DIR / \"Non_standard_NT_image_list.xlsx\"\n",
    "df = pd.read_excel(excel_path)\n",
    "\n",
    "print(f\"Dataset size: {len(df)} images\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare VQA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_cols = [col for col in df.columns if col.startswith('Q')]\n",
    "print(f\"Questions per image: {len(q_cols)}\")\n",
    "\n",
    "vqa_data = []\n",
    "for idx, row in df.head(5).iterrows():\n",
    "    img_path = IMAGE_DIR / row['Image Name']\n",
    "    if not img_path.exists():\n",
    "        continue\n",
    "    \n",
    "    for q_col in q_cols:\n",
    "        question = q_col.split('\\n', 1)[1] if '\\n' in q_col else q_col\n",
    "        question = question[:100]\n",
    "        answer = str(row[q_col])\n",
    "        \n",
    "        if pd.notna(answer) and answer.lower() not in ['nan', 'none', '']:\n",
    "            vqa_data.append({\n",
    "                'image_path': str(img_path),\n",
    "                'question': question,\n",
    "                'answer': answer\n",
    "            })\n",
    "\n",
    "print(f\"Prepared {len(vqa_data)} VQA examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Image Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = Image.open(vqa_data[0]['image_path']).convert('RGB')\n",
    "print(f\"Image size: {test_img.size}\")\n",
    "test_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Florence-2 Model (First Attempt)\n",
    "\n",
    "**Error Encountered**: `'Florence2ForConditionalGeneration' object has no attribute '_supports_sdpa'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft/Florence-2-base\"\n",
    "\n",
    "try:\n",
    "    processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16\n",
    "    ).to(\"cuda\")\n",
    "    print(\"Model loaded successfully!\")\n",
    "except AttributeError as e:\n",
    "    print(f\"❌ ERROR: {e}\")\n",
    "    print(\"\\nThe model doesn't support SDPA (Scaled Dot-Product Attention)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Florence-2 Model (Second Attempt with Eager Attention)\n",
    "\n",
    "**Workaround**: Using `attn_implementation=\"eager\"` to bypass SDPA requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        attn_implementation=\"eager\"  # Workaround for SDPA issue\n",
    "    ).to(\"cuda\")\n",
    "    print(\"Model loaded successfully with eager attention!\")\n",
    "    print(f\"Model device: {next(model.parameters()).device}\")\n",
    "    print(f\"Model dtype: {next(model.parameters()).dtype}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test VQA Inference\n",
    "\n",
    "**Error Encountered**: `'NoneType' object has no attribute 'shape'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What organ is visible in this ultrasound image?\"\n",
    "prompt = f\"<VQA> {question}\"\n",
    "\n",
    "try:\n",
    "    inputs = processor(text=prompt, images=test_img, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
    "    \n",
    "    print(\"Input tensors prepared:\")\n",
    "    for key, value in inputs.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(f\"  {key}: shape={value.shape}, dtype={value.dtype}\")\n",
    "    \n",
    "    print(\"\\nGenerating response...\")\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=100)\n",
    "    \n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    print(f\"\\nGenerated response: {generated_text}\")\n",
    "    \n",
    "except AttributeError as e:\n",
    "    print(f\"\\n❌ ERROR during inference: {e}\")\n",
    "    print(\"\\nThis error suggests a dtype/shape mismatch in the model's forward pass.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ ERROR: {type(e).__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Errors Encountered\n",
    "\n",
    "1. **SDPA Compatibility Issue**:\n",
    "   - Error: `'Florence2ForConditionalGeneration' object has no attribute '_supports_sdpa'`\n",
    "   - Workaround: Use `attn_implementation=\"eager\"`\n",
    "\n",
    "2. **Inference Dtype/Shape Issue**:\n",
    "   - Error: `'NoneType' object has no attribute 'shape'`\n",
    "   - Cause: Internal model forward pass fails, likely due to vision encoder incompatibility\n",
    "\n",
    "### Root Cause Analysis\n",
    "\n",
    "The Florence-2 model has compatibility issues with the current transformers version or requires specific initialization. The errors suggest:\n",
    "\n",
    "1. **Version Mismatch**: The model may require a specific transformers version\n",
    "2. **Custom Code Issues**: The `trust_remote_code=True` flag loads custom model code that may have bugs\n",
    "3. **Preprocessing Requirements**: The model may need specific image preprocessing\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "1. **Alternative Models**: Consider using BLIP-2, LLaVA, or InstructBLIP (more stable)\n",
    "2. **Report Issue**: This appears to be a model compatibility bug\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "**Florence-2 is NOT recommended for this project** due to unstable integration with transformers library. Proceeding with BLIP-2 instead."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
