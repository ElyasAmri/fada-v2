{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaVA-1.5-7B VQA Pipeline Test\n",
    "\n",
    "**Model**: `llava-hf/llava-1.5-7b-hf`  \n",
    "**Task**: Visual Question Answering (VQA) on fetal ultrasound images  \n",
    "**Status**: ❌ FAILED - GPU memory constraints\n",
    "\n",
    "## Problem\n",
    "\n",
    "The LLaVA-1.5-7B model is too large for the RTX 4070 laptop GPU, even with 8-bit quantization.\n",
    "\n",
    "## Hardware\n",
    "\n",
    "- GPU: RTX 4070 (Laptop)\n",
    "- VRAM: ~8GB\n",
    "- Model size: ~7B parameters (~14GB in FP16, ~7GB in 8-bit)\n",
    "\n",
    "## Error\n",
    "\n",
    "```\n",
    "ValueError: Some modules are dispatched on the CPU or the disk. \n",
    "Make sure you have enough GPU RAM to run the model.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Excel annotations\n",
    "DATA_DIR = Path(r\"C:\\Users\\elyas\\Workspace\\PyCharm\\fada-v3\\data\\Fetal Ultrasound Labeled\")\n",
    "IMAGE_DIR = Path(r\"C:\\Users\\elyas\\Workspace\\PyCharm\\fada-v3\\data\\Fetal Ultrasound\\Non_standard_NT\")\n",
    "\n",
    "excel_path = DATA_DIR / \"Non_standard_NT_image_list.xlsx\"\n",
    "df = pd.read_excel(excel_path)\n",
    "\n",
    "print(f\"Dataset size: {len(df)} images\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare VQA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_cols = [col for col in df.columns if col.startswith('Q')]\n",
    "\n",
    "vqa_data = []\n",
    "for idx, row in df.head(5).iterrows():\n",
    "    img_path = IMAGE_DIR / row['Image Name']\n",
    "    if not img_path.exists():\n",
    "        continue\n",
    "    \n",
    "    for q_col in q_cols:\n",
    "        question = q_col.split('\\n', 1)[1] if '\\n' in q_col else q_col\n",
    "        question = question[:100]\n",
    "        answer = str(row[q_col])\n",
    "        \n",
    "        if pd.notna(answer) and answer.lower() not in ['nan', 'none', '']:\n",
    "            vqa_data.append({\n",
    "                'image_path': str(img_path),\n",
    "                'question': question,\n",
    "                'answer': answer\n",
    "            })\n",
    "\n",
    "print(f\"Prepared {len(vqa_data)} VQA examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Image Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = Image.open(vqa_data[0]['image_path']).convert('RGB')\n",
    "print(f\"Image size: {test_img.size}\")\n",
    "test_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Attempt Model Loading (8-bit Quantization)\n",
    "\n",
    "**This cell will fail due to insufficient GPU memory.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "try:\n",
    "    print(\"Loading processor...\")\n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "    \n",
    "    print(\"Loading model with 8-bit quantization...\")\n",
    "    model = LlavaForConditionalGeneration.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        load_in_8bit=True\n",
    "    )\n",
    "    \n",
    "    print(\"Model loaded successfully!\")\n",
    "    print(f\"Device map: {model.hf_device_map}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ FAILED: {type(e).__name__}\")\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    print(\"\\nReason: RTX 4070 laptop GPU does not have enough VRAM for LLaVA-1.5-7B\")\n",
    "    print(\"Even with 8-bit quantization, the model requires ~7-8GB VRAM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test VQA (If Model Loaded)\n",
    "\n",
    "**This cell will not run due to model loading failure.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'model' in locals():\n",
    "    question = \"What organ is shown in this ultrasound image?\"\n",
    "    \n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": f\"Question: {question}\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "    inputs = processor(images=test_img, text=prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=100)\n",
    "    \n",
    "    response = processor.decode(output[0], skip_special_tokens=True)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Response: {response}\")\n",
    "else:\n",
    "    print(\"Model not loaded - skipping VQA test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Why LLaVA-1.5-7B Failed\n",
    "\n",
    "1. **Model Size**: 7 billion parameters\n",
    "2. **Memory Requirements**:\n",
    "   - FP16: ~14GB VRAM\n",
    "   - 8-bit quantization: ~7-8GB VRAM\n",
    "   - 4-bit quantization: ~4-5GB VRAM (quality degradation)\n",
    "3. **Available VRAM**: RTX 4070 laptop has ~8GB, but OS and processes use some\n",
    "4. **Result**: Model cannot fit in GPU memory\n",
    "\n",
    "### Alternative Approaches\n",
    "\n",
    "1. **Smaller VLMs**:\n",
    "   - BLIP-2 with Flan-T5-base (~1B parameters) ✅ SUCCESS\n",
    "   - InstructBLIP with smaller backbones\n",
    "\n",
    "2. **Cloud GPUs**:\n",
    "   - Google Colab with A100/V100\n",
    "   - AWS/Azure with larger GPUs\n",
    "\n",
    "3. **Hybrid Approach (Recommended)**:\n",
    "   - Phase 1: Classification + template responses\n",
    "   - Phase 2: Add smaller VLM when dataset grows\n",
    "\n",
    "### Decision\n",
    "\n",
    "Proceeding with BLIP-2 as it fits in available GPU memory and provides acceptable VQA performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
