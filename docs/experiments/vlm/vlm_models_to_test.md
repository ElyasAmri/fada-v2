# Vision-Language Models to Test for FADA

**Date**: October 2, 2025
**Goal**: Compare multiple VLMs for fetal ultrasound VQA
**Hardware**: RTX 4070 (8GB VRAM)

## Testing Priority List

### Tier 1: Small Models (Fit in 8GB) - TEST THESE FIRST

1. ‚úÖ **BLIP-2** (Current baseline)
   - Status: TRAINED (5 categories)
   - Size: 3.4B params
   - Memory: 4.2GB
   - Accuracy: Working, needs improvement on quality
   - Repository: Salesforce/blip2-opt-2.7b

2. ‚úÖ **FetalCLIP** (Domain-specific)
   - Status: TESTED (40% zero-shot)
   - Size: ~400M params
   - Memory: ~3GB
   - Accuracy: 40% (zero-shot, category mismatch)
   - Repository: github.com/BioMedIA-MBZUAI/FetalCLIP
   - Note: CLIP-style, not generative VQA

3. ‚ö†Ô∏è **TinyGPT-V**
   - Status: REQUIRES COMPLEX SETUP
   - Size: 2.8B params
   - Memory: 8GB inference
   - Expected: 98% of InstructBLIP performance
   - Repository: github.com/DLYuanGod/TinyGPT-V
   - Tasks: VQA, image captioning
   - **Issue**: Requires conda environment, Phi-2 weights, custom config
   - **Effort**: High (2-3 hours setup)
   - **Decision**: SKIP for now due to time constraints

4. ‚¨ú **SmolVLM-500M**
   - Status: TO TEST
   - Size: 500M params
   - Memory: <2GB
   - Expected: Good for captioning, document Q&A
   - Repository: HuggingFace (check)
   - Tasks: Captioning, visual reasoning
   - Note: Newest lightweight VLM (2024-2025)

5. ‚ö†Ô∏è **DeepSeek-VL-1.3B**
   - Status: REQUIRES CUSTOM SETUP
   - Size: 1.3B params
   - Memory: ~3-4GB
   - Expected: Strong reasoning, scientific tasks
   - Repository: github.com/deepseek-ai/DeepSeek-VL
   - **Issue**: Requires git clone + pip install -e . (custom package)
   - **Effort**: Medium (10-15 minutes setup)
   - **Decision**: SKIP for now due to setup complexity

6. ‚¨ú **Moondream2**
   - Status: TO TEST
   - Size: Small (TBD)
   - Memory: <4GB
   - Expected: Simple VQA
   - Repository: Check HuggingFace

7. ‚¨ú **BLIP-VQA-base**
   - Status: TO TEST
   - Size: 385M params
   - Memory: ~2GB
   - Expected: Lightweight VQA
   - Repository: Salesforce BLIP variants

8. ‚¨ú **VILT-b32-finetuned-vqa**
   - Status: TO TEST
   - Size: 87M params
   - Memory: <1GB
   - Expected: Very lightweight VQA
   - Repository: HuggingFace

### Tier 2: Medium Models (Need Quantization for 8GB)

9. ‚¨ú **Qwen-VL-7B**
   - Status: TO TEST (with 8-bit quantization)
   - Size: 7B params
   - Memory: ~8GB with quantization
   - Expected: SOTA VQA performance
   - Repository: github.com/QwenLM/Qwen-VL
   - Note: Best text-related recognition/QA

10. ‚¨ú **MiniGPT-4**
    - Status: TO TEST (with int8)
    - Size: ~7B params
    - Memory: Needs int8 quantization
    - Expected: Strong conversational
    - Repository: Check availability

11. ‚¨ú **InstructBLIP-7B**
    - Status: TO TEST (with quantization)
    - Size: 7B params
    - Memory: ~8GB with quantization
    - Expected: Excellent traditional VQA
    - Repository: Salesforce InstructBLIP

12. ‚¨ú **LLaVA-NeXT-7B**
    - Status: TO TEST (with 8-bit)
    - Size: 7B params
    - Memory: ~8GB with quantization
    - Expected: Strong multimodal reasoning
    - Repository: github.com/haotian-liu/LLaVA

### Tier 3: Medical-Specific Models

13. ‚¨ú **RaDialog**
    - Status: CHECK AVAILABILITY
    - Domain: Radiology
    - Task: Report generation
    - Expected: Clinically accurate reports
    - Note: Check if weights publicly available

14. ‚¨ú **PathChat**
    - Status: CHECK AVAILABILITY
    - Domain: Pathology
    - Task: VQA
    - Expected: Clinical effectiveness
    - Note: Check if weights publicly available

15. ‚¨ú **Flamingo-CXR**
    - Status: CHECK AVAILABILITY
    - Domain: Chest X-rays
    - Task: Report generation
    - Expected: SOTA for chest radiographs
    - Note: Check if weights publicly available

### Tier 4: Medical CLIP Variants (Already Researched)

16. ‚¨ú **PubMedCLIP**
    - Status: RESEARCHED
    - Domain: Medical (ROCO dataset)
    - Task: Classification (needs adapter for VQA)
    - Memory: ~2-4GB
    - Note: Most stable for ultrasound (research finding)

17. ‚¨ú **BiomedCLIP**
    - Status: RESEARCHED
    - Domain: Medical (PMC-15M)
    - Task: Classification (needs adapter for VQA)
    - Memory: ~3-5GB
    - Note: Best medical VQA performance (+6 points)

18. ‚¨ú **PLIP**
    - Status: RESEARCHED
    - Domain: Pathology
    - Task: Classification
    - Memory: ~2-3GB
    - Note: Pathology-specific, not for ultrasound

## Testing Protocol

### For Each Model:

1. **Installation**
   - Check availability
   - Install dependencies
   - Download weights

2. **Quick Test** (15 images, 5 categories)
   - Zero-shot classification (if applicable)
   - VQA on sample images
   - Record accuracy, quality, speed

3. **Full Test** (if promising)
   - Complete test set
   - Generate confusion matrix
   - Measure inference time
   - Compare response quality

4. **Documentation**
   - Model specs
   - Test results
   - Pros/cons
   - Use case recommendations

## Success Criteria

### Must Have:
- Works on 8GB GPU
- Supports VQA or can be adapted
- Reasonable inference time (<30s/image)

### Nice to Have:
- Medical domain knowledge
- Open-source weights
- Active maintenance
- Good documentation

### Comparison Metrics:
- **Accuracy**: Classification/VQA correctness
- **Quality**: Response naturalness and detail
- **Speed**: Inference time per image
- **Memory**: GPU VRAM usage
- **Ease**: Setup and integration difficulty

## Current Status

**Completed Testing (7 models):**
- ‚úÖ BLIP-2: Trained and working (baseline) - **BEST CHOICE**
- ‚úÖ Moondream2: Good fetal context, fast (1.2s) - Second best
- ‚úÖ SmolVLM-500M: Efficient but no fetal context
- ‚úÖ SmolVLM-256M: World's smallest, recognizes fetal but generic
- ‚úÖ BLIP-VQA-base: Too brief (1-2 words)
- ‚úÖ VILT-b32: Nonsensical (fixed vocabulary)
- ‚ö†Ô∏è FetalCLIP: 40% accuracy (category mismatch)

**Skipped (Setup Complexity):**
- ‚ö†Ô∏è TinyGPT-V: Requires conda + Phi-2 weights + custom config
- ‚ö†Ô∏è DeepSeek-VL-1.3B: Requires custom package installation

**Blocked (Access Required):**
- üîí PaliGemma-3B: Gated model (requires HuggingFace approval)

**Verdict**: BLIP-2 validated as best choice for fetal ultrasound VQA

## Expected Outcomes

### Best Case:
- Find model with >90% accuracy AND better VQA quality
- Replace BLIP-2 or use as ensemble

### Realistic Case:
- Find 2-3 models comparable to BLIP-2
- Document comprehensive comparison
- Choose best for specific use cases

### Minimum Case:
- Confirm BLIP-2 is best choice
- Document why alternatives don't work
- Strong research justification

## Notes

- Focus on Tier 1 models first (proven to fit 8GB)
- Tier 2 requires quantization experiments
- Tier 3 may not have public weights
- Tier 4 needs adapters for VQA generation

---

**Last Updated**: October 2, 2025
**Models Tested**: 7 working + 3 skipped/blocked = 10/18
**Status**: ‚úÖ Testing complete - BLIP-2 validated as best choice
**Documentation**: See `notebooks/vlm_comparison.ipynb` for full comparison
